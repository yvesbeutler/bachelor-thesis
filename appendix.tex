\chapter{Resources}

This chapter contains code snippets, performance measurements, and additional information which would unnecessarily bloat the report. Only the important parts are covered in this section. The complete source code was handed to the supervising professor before the submission deadline.

\section{Performance Metrics}

The key performance indicators are measured with a total of 967 manually-labelled messages. These numbers will be affected by adding more training samples. Consider section \ref{chap:formulas} for more information about the different KPIs.

\subsection{SpaCy}

\begin{table}[ht!]
    \centering
    \begin{tabular}{|p{6em}|p{3em}|}
        \hline
        Accuracy & 0.946 \\
        \hline
        Precision & 0.444 \\
        \hline
        Recall & 0.533 \\
        \hline
        \textbf{F1 Score} & \textbf{0.485} \\
        \hline
    \end{tabular}
    \caption{Spacy KPIs}
    \label{tbl:perf-spacy}
\end{table}

\begin{table}[ht!]
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|p{6em}|p{3em}|}
            \hline
            Accuracy & 0.955 \\
            \hline
            Precision & 0.405 \\
            \hline
            Recall & 0.685 \\
            \hline
            \textbf{F1 Score} & \textbf{0.509} \\
            \hline
        \end{tabular}
        \caption{Spacy (only PER)}
        \label{tbl:perf-spacy-per}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|p{6em}|p{3em}|}
            \hline
            Accuracy & 0.933 \\
            \hline
            Precision & 0.039 \\
            \hline
            Recall & 0.158 \\
            \hline
            \textbf{F1 Score} & \textbf{0.063} \\
            \hline
        \end{tabular}
        \caption{Spacy (only LOC)}
        \label{tbl:perf-spacy-loc}
    \end{minipage}
\end{table}

\subsection{Basic Lookup Model}

Consider chapter \ref{chap:regex-model} for more information about the different enhancements.

\begin{table}[ht]
    \centering
    \begin{tabular}{|L{16em}|C{3em}|C{3em}|C{3em}|C{3em}|}
        \hline
        \textbf{Enhancement} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\ [1ex]
        \hline
        1 - Add dictionary lookup & 0.774 & 0.146 & 0.733 & 0.243 \\ [0.2ex]
        \hline
        2 - Remove last names dictionary & 0.953 & 0.582 & 0.401 & 0.475 \\ [0.2ex]
        \hline
        3 - Clean up last names & 0.948 & 0.432 & 0.723 & 0.541 \\ [0.2ex]
        \hline
        4 - Add uppercase constraint & 0.953 & 0.487 & 0.722 & 0.582 \\ [0.2ex]
        \hline
        5 - Escape diacritics & 0.953 & 0.486 & 0.736 & 0.585 \\ [0.2ex]
        \hline
        6 - Add variants (e.g. umlauts) & 0.952 & 0.503 & 0.758 & 0.605 \\ [0.2ex]
        \hline
        7 - Add previous and next tokens & 0.954 & 0.504 & 0.761 & 0.606 \\ [0.2ex]
        \hline
        8 - Clean up first- and last names & 0.982 & 0.773 & 0.752 & 0.762 \\ [0.2ex]
        \hline
        9 - Replace addresses with regex & 0.98 & 0.848 & 0.732 & \textbf{0.786} \\ [0.2ex]
        \hline
    \end{tabular}
    \caption{Basic Lookup Model KPIs}
    \label{tbl:perf-regex}
\end{table}

\begin{table}[ht!]
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|p{6em}|p{3em}|}
            \hline
            Accuracy & 0.984 \\
            \hline
            Precision & 0.737 \\
            \hline
            Recall & 0.885 \\
            \hline
            \textbf{F1 Score} & \textbf{0.804} \\
            \hline
        \end{tabular}
        \caption{Lookup Model (PER)}
        \label{tbl:perf-regex-per}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{tabular}{|p{6em}|p{3em}|}
            \hline
            Accuracy & 0.951 \\
            \hline
            Precision & 0.111 \\
            \hline
            Recall & 0.334 \\
            \hline
            \textbf{F1 Score} & \textbf{0.166} \\
            \hline
        \end{tabular}
        \caption{Lookup Model (LOC)}
        \label{tbl:perf-regex-loc}
    \end{minipage}
\end{table}

\subsection{Deep Learning}

The different models and their parameters are described in chapter \ref{chap:deep-performance}. The best scores are highlighted with bold numbers.

\begin{table}[ht]
    \centering
    \begin{tabular}{|L{16em}|C{3em}|C{3em}|C{3em}|C{3em}|}
        \hline
        \textbf{Deep Learning Approach} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\ [1ex]
        \hline
        1 - Small MLP (323 parameters) & 0.95 & 0.489 & 0.247 & 0.328 \\ [0.2ex]
        \hline
        2 - Large MLP (69123 parameters) & 0.95 & 0.48 & 0.276 & 0.351 \\ [0.2ex]
        \hline
        3 - Large MLP with weights & 0.949 & 0.473 & 0.279 & 0.351 \\ [0.2ex]
        \hline
        4 - Custom embeddings (Mobi24) & 0.983 & 0.902 & 0.73 & 0.807 \\ [0.2ex]
        \hline
        5 - Custom embeddings \& dropouts & 0.986 & 0.944 & 0.755 & 0.839 \\ [0.2ex]
        \hline
        6 - Wiki embeddings (100 dim) & 0.994 & \textbf{0.953} & 0.933 & 0.943 \\ [0.2ex]
        \hline
        7 - Wiki embeddings (300 dim) & 0.995 & 0.951 & 0.947 & 0.949 \\ [0.2ex]
        \hline
        8 - Combined embeddings (300 dim) & 0.995 & 0.948 & \textbf{0.957} & \textbf{0.953} \\ [0.2ex]
        \hline
    \end{tabular}
    \caption{Performance KPIs of the deep learning models}
    \label{tbl:perf-mlp}
\end{table}


\section{Source Code}

\subsection{Keras Tokenizer}

The Keras tokenizer allows you to define the \emph{OOV} token during initialisation. With the \verb|filter| option you can define which characters the tokenizer should filter from the tokens. The default filter would handle "\emph{Hello}" and "\emph{Hello,}" as the same token. For not losing punctuation the \verb|filter| parameter needs to be set to an empty string.

\begin{lstlisting}[language=Python, label={code:keras-tokenizer}, caption=Fitting the Keras tokenizer]
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(oov_token='<OOV>', filters='')

# create a vocabulary based on the training set
tokenizer.fit_on_texts(train_sentences)

# create sequences of IDs
sequences = tokenizer.texts_to_sequences(test_sentences)
\end{lstlisting}

\subsection{Sliding Window}

\begin{lstlisting}[language=Python, label={code:sliding-window}, caption=Sliding window implementation]
def sliding_window(sequences, window_size=5, placeholder='', flatten=True):
    """
    Creates windows of the exact same size, first and last windows
    will be padded with a placeholder. The relevant token of each
    window is placed in the middle. Flattens the list by default.
    """
    half = int(window_size / 2)
    results = []

    for sequence in sequences:

        tokens = sequence[0]
        labels = sequence[1]

        length = len(tokens)
        windows = []

        # create windows
        for idx in range(length):
            start = idx - half
            stop = idx + 1 + half
            windows.append([tokens[max(0, start):stop], labels[idx]])

        # add padding for first windows
        for idx, val in enumerate(range(half, 0, -1)):
            tmp = windows[idx][0]
            for i in range(val):
                tmp = [placeholder] + tmp

            windows[idx][0] = tmp

        # add padding for last windows
        for val, idx in enumerate(range(length-half, length)):
            tmp = windows[idx][0]
            for i in range(val+1):
                tmp = tmp + [placeholder]

            windows[idx][0] = tmp

        results.append(windows)

    if flatten:
        results = [item for sublist in results for item in sublist]

    return results
\end{lstlisting}

\subsection{Creating Embeddings}

\begin{lstlisting}[language=Python, label={code:embeddings}, caption=Creating custom embeddings]
from gensim.models import Word2Vec

DIM_SIZE = 20
WIN_SIZE = 10
MIN_COUNT = 5

w2v = Word2Vec(messages, size=DIM_SIZE, window=WIN_SIZE, min_count=MIN_COUNT, iter=10, workers=multiprocessing.cpu_count())
\end{lstlisting}

\section{Miscellaneous}

\subsection{Excluded Ambiguous Names}
\label{lst:excluded-names}

The following list shows all values which aren't common names or which may be names but are ambiguous. These values are removed from the naming dictionaries which are used by the regex baseline model.

\begin{multicols}{4}
    \begin{itemize}
        \item mobiliar
        \item mobi
        \item immobilien
        \item safari
        \item franken
        \item guten
        \item leider
        \item min
        \item link
        \item weiss
        \item herr
        \item app
        \item kunde
        \item dame
        \item damen
        \item frau
        \item gerne
        \item sport
        \item firma
        \item herren
        \item liebe
        \item schaden
        \item grund
        \item person
        \item kinder
        \item mio
        \item juni
        \item bern
        \item mail
        \item post
        \item mar
        \item sun
        \item fall
        \item ort
        \item hand
        \item mon
        \item wed
        \item may
        \item gruss
        \item abend
        \item mai
        \item juni
        \item juli
        \item phone
        \item spray
        \item buchung
        \item freitag
        \item montag
        \item monday
        \item fehler
        \item glueck
        \item uri
        \item seite
        \item auto
        \item pin
        \item biel
        \item you
        \item helvetia
        \item weg
        \item art
        \item termine
        \item chance
        \item merci
        \item stecker
        \item preis
        \item partner
        \item restaurant
        \item handy
        \item zurich
        \item schlechten
        \item privat
        \item frische
        \item kan
        \item name
    \end{itemize}
\end{multicols}

