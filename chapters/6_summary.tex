\chapter{Summary}

The last chapter of this bachelor thesis focuses on reviewing the project and summarising the findings. It reveals what happens next with the deep learning model and how it could be further optimised.

\section{Reflection}

For humans it might be irritating to understand the issues computers can have in understanding natural language. After dealing with my own pre-processed data for more than four months, I can totally understand these poor little robots. Language is very versatile, ambiguous and even flexible, especially if humans don't pay too much attention on spelling.
After developing the baseline lookup model, I was worried about its respectable performance and if I was able to beat it with deep learning. Loosing against the baseline wouldn't devalue this bachelor thesis, but definitely hurt my pride as a data scientist.

\subsection{Lessons Learned}

During the last months I learned a lot about data science. I appreciate these lessons learned and think that mistakes you make are much more valuable than simply reading about experiences from others. As mentioned in section \ref{chap:deep-performance}, it is a bit devastating that things you learnt in theory don't apply in practice. This gave me an important insight about the work as a data scientist. You have to consider every problem individually. There is no such thing as a one-size-fits-all solution. It's often challenging to find answers about why an established concept works or doesn't work with my data.

I saw how time-consuming data cleansing can be. I have read several articles on this subject that said that data preprocessing takes up to 80\% of a data scientist's time, but I could not imagine this is true \cite{naman18}. After spending more than one month with preprocessing tasks I have to reconsider. I learned that a sound database is key to work with. In the first hour of data labelling, I didn't come across a single message containing junk, spam or any foreign language. This strengthened my belief that I did a good job at cleansing.

One aspect I totally underestimated was the performance measuring. Because of my previous classes I was aware of the existing key performance indicators and how to use them. Therefore, after creating the model I thought a simple part would begin. But I was wrong and performance measuring is not trivial. The test data needs to be in different shapes. For a model working with embeddings you have to map the test data also to word vectors. For the tokenised models the input needs to be tokenised and split into windows. You even have to define what happens if a model recognises a token as named entity \emph{LOC} but it's actually \emph{PER}. I decided to handle these situations as false positives\footnote{For the missed out \emph{PER}, a false negative would be correct too}.

In a future project, I would definitely spend less time in refactoring code for exploratory tasks which doesn't need to be robust. Exploring data is a very agile and fast task. I could have spent less time on these works if I didn't parameterise every piece of code or tried to generalise methods. Sometimes my experience as a software developer gets in my way when doing data science. On the other hand I'm very glad that I know how to develop software. Due to my knowledge it was very simple to release code in form of python libraries or work with the \emph{teamcity} build server.

\subsection{Motivation}

While working on this bachelor thesis there have been multiple key moments which kept me motivated. A very surprising aspect was, that I started to like labelling data. Many colleagues told me, it's a Sisyphean task which every data scientist has to do from time to time. I labelled my data in shifts and not more than one hour per session. I used this not so demanding work to make a pause or to get a distance between my mind and the work I've done before. This was especially useful when I was struggling with programming issues or couldn't think of a solution to a certain problem. Many times I returned to the previous task and had a bunch of new ideas what to try next.

A very motivating fact was, that I could act as a test user for my scrum team while progressing on my project. There were several test cases where my model or the data I used have been involved. I gave feedback to the so-called model project, which is basically a template that allows data scientist to deploy their models for using at Swiss Mobiliar. I uploaded my original data, the cleansed, and the labelled version of it to the data repository. The data repository is a self-made tool which allows sharing and versioning of data sets.

The baseline's performance maybe pushed me the most to develop an even better deep learning model. First I thought it was very challenging, but with the use of embeddings I overpowered the dictionary lookup model at first try. Seeing your model growing and increasing its performance is very exciting.

\section{Further Improvements}

After finishing this bachelor thesis, the NER project at Swiss Mobiliar doesn't end. There is a lot of work to do, before the model can be used in production. And there are some very promising concepts which I couldn't find the time to try out yet.

\subsection{Expand the Data}

I had to deal with a lot of overfitting. The best solution for this problem is to increase the training data size. But getting reliable data is one of the toughest problems in data science. Luckily one of the data scientists at Swiss Mobiliar is currently testing external labelling partners. For demonstration purposes they label several hundred messages of the \emph{Mobi24} dataset. If the data passes the gold standard, it will be used for training. A possible partnership with the company could result in much more data which would hopefully improve the generalisation capabilities of the model.

\subsection{Add Recurrent Layers}

The used multi-layer perceptron is a simple feed forward network. The neurons pass their data to the neurons in the next layer. Recurrent neural networks (RNN) are different to feed forward networks. Their neurons can be connected to neurons in the same or previous layer. This allows the network to process sequences with a given context. This is appreciated in tasks like speech recognition and especially natural language processing \cite{wiki07}. A specific RNN architecture, using a memory called \emph{cell state}, is the long short-term memory (LSTM). This memory helps keeping a relation between previous and next elements in a sequence. More information and the three gates used by a LSTM network are best described in its original paper of 1997 \cite{lstm97}. It is very promising that the use of recurrent layers will increase the model's performance. Due to limited time resources this will be tried out after the bachelor thesis ends.

\section{Acknowledgements}

I'd like to express my gratitude to my supervising professor Dr. JÃ¼rgen Vogel and the expert Pierre-Yves Voirol for keeping the project and especially this report on track. Special thanks to my supervisor at Swiss Mobiliar, Marcus Schwemmle, for his support during the last few months. I'm very happy that he and the other data scientists shared their vast knowledge with me.
